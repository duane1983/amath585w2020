{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preconditioned Conjugage Gradient Method\n",
    "\n",
    "\n",
    "[AMath 585, Winter Quarter 2020](http://staff.washington.edu/rjl/classes/am585w2020/) at the University of Washington. Developed by R.J. LeVeque and distributed under the [BSD license](https://github.com/rjleveque/amath585w2020/blob/master/LICENSE).  You are free to modify and use as you please, with attribution.\n",
    "\n",
    "These notebooks are all [available on Github](https://github.com/rjleveque/amath585w2020/).\n",
    "\n",
    "-----\n",
    "\n",
    "This notebook corrects some errors in Section 4.3.5 of the text, and includes some more discussion of PCG methods.\n",
    "\n",
    "For an example implementation and application, see [DarcyFlow.ipynb](DarcyFlow.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction of errors\n",
    "\n",
    "The preconditioned conjugate gradient method on p. 95 of the text contains some errors.\n",
    "\n",
    "The lines defining $\\alpha_{k-1}$ and $\\beta_k-1$ are incorrect and should read:\n",
    "\n",
    "$$\n",
    "\\alpha_{k-1} = (z_{k-1}^T r_{k-1}) / (p_{k-1}^T w_{k-1})\n",
    "\\quad\\text{($z$ instead of $r$ in the numerator)}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\beta_{k-1} = (z_k^T r_k) / (z_{k-1}^T r_{k-1})\n",
    "\\quad\\text{($z$ instead of $r$ in two places)}\n",
    "$$\n",
    "\n",
    "There are other typos in describing the relation between variables earlier on the page. The correct expressions are:\n",
    "\n",
    "$$\n",
    "w_k = C^T \\tilde w_k \\qquad\\text{and}\\qquad r_k = C^T(\\tilde f - \\tilde A \\tilde u_k)\n",
    "= f - A u_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea\n",
    "\n",
    "The main idea of how the preconditioner is applied is described in the text in more detail, but will be summarized again here.\n",
    "\n",
    "We wish to solve the linear system\n",
    "\n",
    "$$\n",
    "Au = f\n",
    "$$\n",
    "\n",
    "in a case where $A$ is symmetric positive (or negative) definite so that the conjugate gradient algorithm can be used, but is ill-conditioned, in which case CG might not converge as quickly as we would like.\n",
    "\n",
    "We choose $M$ as some approximation to $A$ for which it is cheap to solve systems of the form \n",
    "\n",
    "$$\n",
    "Mz = r,\n",
    "$$\n",
    "\n",
    "and we solve such a system in each iteration and then use $z$ in place of $r$ in several places.  \n",
    "\n",
    "Then the preconditioned C-G  (PCG) algorithm is equivalent to applying CG to a different system that has the same solution as the original problem, but with condition number given by that of $M^{-1}A$ rather than the condition number of $A$.   The better $M$ approximates $A$, the smaller this condition number will be (and the faster the algorithm converges).\n",
    "\n",
    "We must also choose $M$ to be symmetric positive definite.  This is because the PCG algorithm is actually derived by applying CG to the modified system\n",
    "\n",
    "$$\n",
    "(C^{-T}AC^{-1})(Cu) = C^{-T}f, \\qquad\\text{ or}\\quad \\tilde A \\tilde u = \\tilde f\n",
    "$$\n",
    "\n",
    "where $C$ is a matrix satisfying $C^TC = M$.  Since $C^TC$ is always symmetric positive definite (provided $C$ is nonsingular), we are implicitly assuming that $M$ is.\n",
    "\n",
    "We apply C-G to this system rather than to $M^{-1}Au = M^{-1}f$ because the matrix $M^{-1}A$ might not be symmetric, even if $A$ and $M$ are, and so C-G would not be applicable.  \n",
    "\n",
    "Moreover, the matrix $\\tilde A = C^{-T}AC^{-1}$ is always symmetric positive definite (as long as $A$ is nonsingular) since for any nonzero vector $u$,\n",
    "\n",
    "$$\n",
    "u^T\\tilde A u = (C^{-1}u)^T A (C^{-1}u) > 0\n",
    "$$\n",
    "\n",
    "provided $A$ is SPD.\n",
    "\n",
    "Also, $\\tilde A$ has the same 2-norm condition number as $M^{-1}A$, since they are similar and so have the same eigenvalues.  See the text and references for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky factorization\n",
    "\n",
    "The matrix $C$ is never actually computed but implicitly exists provided $M$ is SPD. One possible $C$ is obtained by computing the Cholesky decomposition of $M$, which is like the LU factorization of $M$ obtained with Gaussian Elimination, but with a more symmetric form in the SPD case.  In this case it can be shown that the diagonal elements of $U$ are always positive, and by symmetry if you factor a matrix $D = \\text{diag}(U_{ii})$ out of $U$, you obtain:\n",
    "\n",
    "$$\n",
    "LU = LDL^T.\n",
    "$$\n",
    "\n",
    "Now set $C = D^{1/2}L^T = \\text{diag}(\\sqrt{U_{ii}})L^T$ and you have $M = LU = C^TC$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal preconditioning\n",
    "\n",
    "One simple (but often effective) preconditioner in which $M$ is the diagonal part of $A$.  For a constant coefficient Poisson problem this would be a constant multiple of the identity matrix and so all the eigenvalues would be scaled by the same factor and hence $M^{-1}A$ would have the same condition number as $A$. \n",
    "\n",
    "So the diagonal preconditioner is of no use for the Poisson problem, but for the variable coefficient problem it can help a lot, particularly if the coefficients vary greatly.\n",
    "This is illustrated in the notebook [DarcyFlow.ipynb](DarcyFlow.ipynb).\n",
    "\n",
    "Note that if $A$ is SPD then we should use $M = \\text{diag}(A_{ii})$ as the diagonal preconditioner.  On the other hand if $A$ is SND (symmetric negative definite) then we should instead use $M = \\text{diag}(-A_{ii})$, so that this is a SPD matrix.  In this case the resulting $\\tilde A$ will still be SND.  (Note: It is a theorem that any SPD matrix has positive diagonal elements while any SND matrix has negative diagonal elements.) \n",
    "\n",
    "In either case we have $M = \\text{diag}(|A_{ii}|)$ and the matrix $C$ can be $C = \\text{diag}(\\sqrt{|A_{ii}|})$.\n",
    "\n",
    "As a simple example, suppose $A$ is a diagonal matrix with all positive values on the diagonal, and hence is SPD with 2-norm condition number $\\max(A_{ii})/\\min(A_{ii})$ which could be arbitrarily large depending on our choice of diagonal elements, and so convergence could be quite slow.  In this case, diagonal preconditioning would work great: $\\tilde A = I$, the identity matrix, and so the PCG algorithm would converge in one iteration (since all the eigenvalues are equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
