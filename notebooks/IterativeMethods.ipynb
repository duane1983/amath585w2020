{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative methods\n",
    "\n",
    "[AMath 585, Winter Quarter 2020](http://staff.washington.edu/rjl/classes/am585w2020/) at the University of Washington. Developed by R.J. LeVeque and distributed under the [BSD license](https://github.com/rjleveque/amath585w2020/blob/master/LICENSE).  You are free to modify and use as you please, with attribution.\n",
    "\n",
    "These notebooks are all [available on Github](https://github.com/rjleveque/amath585w2020/).\n",
    "\n",
    "-----\n",
    "\n",
    "This notebook illustrates some classical iterative methods based on matrix splitting: Jacobi, Gauss-Seidel, and SOR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better illustrate the sequence of approximate solutions obtained as we iterate, we will use a widget to display a sequence of plots in a way that we can easily look through them.  Other available widgets are described and illustrated in the [ipywidgets documentation](https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Basics.html).\n",
    "\n",
    "The function `animate_figs` defined below invokes an iteractive widget to loop over a set of matplotlib figures that have been generated in some loop and stored as a list called `figs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def animate_figs(figs):\n",
    "    show_frame = lambda frameno: display(figs[frameno])\n",
    "    interact(show_frame, frameno=widgets.IntSlider(min=0,max=len(figs)-1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a simple BVP in 1D\n",
    "\n",
    "For illustration we will solve $u''(x) = f(x)$ in $0\\leq x \\leq 1$ with Dirichlet boundary conditions.\n",
    "\n",
    "We choose a cubic function $u(x)$ as the solution so that the truncation error is zero, and hence the exact solution of the linear system should be the exact solution of the ODE at the grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fcn = lambda x: 6*x + 2\n",
    "utrue_fcn = lambda x: x**3 + x**2 - x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `solve_bvp` function sets up and solves the linear system by an iterative method that is defined by the argurment `update_u`, which should be a function to take a single step of some iterative method.  \n",
    "\n",
    "This allows us to easily test different methods by passing in a different `update_u` function, without repeating all the other stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is set up to solve the system in which the tridiagonal matrix has been augmented by a row of the identity matrix at the top and bottom with right hand sides values given by the Dirichlet boundary conditions, i.e., the first and last equations in the system are `U[0] = alpha` and `U[m+1] = beta`.  This is convenient for plotting purposes and also if you want to try modifying the problem to use Neumann BCs at one of the boundaries, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_bvp(f_fcn, utrue_fcn, m, update_u, maxiter, kplot):\n",
    "\n",
    "    alpha = utrue_fcn(0)\n",
    "    beta = utrue_fcn(1)\n",
    "\n",
    "    h = 1./(m+1)\n",
    "    x = linspace(0,1,m+2)\n",
    "\n",
    "    utrue = utrue_fcn(x)\n",
    "    F = f_fcn(x)\n",
    "\n",
    "    U0 = linspace(alpha, beta, m+2) # initial guess\n",
    "    U = U0.copy() # current iterate\n",
    "\n",
    "    max_error = abs(U-utrue).max()\n",
    "    errors = [max_error]\n",
    "\n",
    "    figs = []  # for the list of figures we generate\n",
    "    \n",
    "    for k in range(1,maxiter+1):\n",
    "                \n",
    "        U = update_u(U, F, h)  # take one iteration\n",
    "        \n",
    "        max_error = abs(U-utrue).max()\n",
    "        errors.append(max_error)\n",
    "\n",
    "        if mod(k,kplot)==0 or k==maxiter:\n",
    "            # every kplot iterations create a plot:\n",
    "            fig = figure(figsize=(12,5))\n",
    "            plot(x,U0,'r-o', label='initial guess')\n",
    "            plot(x,utrue,'k-o', label='true solution')\n",
    "            plot(x,U,'bo-', label= 'iteration k = %i' % k)\n",
    "            legend()\n",
    "            grid(True)\n",
    "            xlim(0,1)\n",
    "            ylim(0, 3)\n",
    "            title('After %i iterations, max error = %.2e' \\\n",
    "                  % (k, max_error))\n",
    "            figs.append(fig)\n",
    "            close(fig)\n",
    "            \n",
    "    errors = array(errors)  # convert list to numpy array\n",
    "    return errors, figs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi iteration\n",
    "\n",
    "This function does one update step for Jacobi.  Note that the way we have defined the system to include the boundary values, Jacobi does not change the first or last element of `U`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_u_Jacobi(U, F, h):\n",
    "    \"\"\"\n",
    "    Input: Current iterate U^[k-1]\n",
    "    Output: Next iterate U^[k]\n",
    "    \"\"\"\n",
    "    m = len(U) - 2\n",
    "    Uprev = U.copy()  # save current iterate so it's not overwritten\n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        U[i] = 0.5*(Uprev[i-1] + Uprev[i+1] - h**2 * F[i])\n",
    "        \n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 19\n",
    "h = 1./(m+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Jacobi\n",
    "\n",
    "If you adjust this to plot every iteration for say 20 iteration you will see that convergence is very slow.  Here we plot every 20 iterations for 400:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_Jacobi, figs = solve_bvp(f_fcn, utrue_fcn, m=m, update_u=update_u_Jacobi, \n",
    "                                maxiter=400, kplot=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_figs(figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Seidel iteration\n",
    "\n",
    "For Gauss-Seidel we update `U[i]` using the already-updated `U[i-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_u_GS(U, F, h):\n",
    "    \"\"\"\n",
    "    Input: Current iterate U^[k-1]\n",
    "    Output: Next iterate U^[k]\n",
    "    \"\"\"\n",
    "    m = len(U) - 2\n",
    "    for i in range(1,m+1):\n",
    "        U[i] = 0.5*(U[i-1] + U[i+1] - h**2 * F[i])\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_GS, figs = solve_bvp(f_fcn, utrue_fcn, m=m, update_u=update_u_GS, \n",
    "                            maxiter=400, kplot=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_figs(figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that Gauss-Seidel converges faster, as expected.  If we plot the error vs. k, we can see that both are decaying exponentially, though slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(errors_Jacobi, 'r', label='Jacobi')\n",
    "plot(errors_GS, 'b', label='Gauss-Seidel')\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we expect the error to decay like $E(k) \\approx C \\rho^k$ for some convergence rate $\\rho$, the rates are clearer if we make a semilogy plot.  Since we expect $\\log(E(k)) \\approx \\log(C) + k\\log(\\rho)$ we expect the error to decay linearly in such a plot with slope $\\log(\\rho) < 0$ since $\\rho < 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogy(errors_Jacobi, 'r', label='Jacobi')\n",
    "semilogy(errors_GS, 'b', label='Gauss-Seidel')\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the convergence rate\n",
    "\n",
    "Since we expect $\\log(E(k)) \\approx \\log(C) + k\\log(\\rho)$ and the convergence looks so linear, we could fit a straight line through a couple points on these plots and easily estimate $\\log(\\rho)$.  \n",
    "\n",
    "A more general approach, if the convergence is a bit ragged, would be to do a least squares fit of a linear function to more points (say $n$) from this convergence plot, setting up a linear system using $\\log(E_i) = c_1 + c_2 k_i$ for $i=1,~2,~\\ldots,~n$,\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc}\n",
    "1 & k_1\\\\\n",
    "1 & k_2\\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & k_n\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "c_1\\\\\n",
    "c_2\n",
    "\\end{array}\\right]\n",
    "=\n",
    "\\left[\\begin{array}{c}\n",
    "\\log(E_1)\\\\\n",
    "\\log(E_2)\\\\\n",
    "\\vdots\\\\\n",
    "\\log(E_n)\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "and then solving this in the least squares sense for $[c_1,~c_2]$.  Then $\\rho \\approx \\exp(c_2)$.  \n",
    "\n",
    "Here we explicitly set up and solve this least square problem using [numpy.linalg.lstsq](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_rate(k_vals, errors):\n",
    "    n = len(k_vals)\n",
    "    assert len(errors) == n, 'k_vals and errors must have the same length'\n",
    "    print('Estimating rate based on %i values' % n)\n",
    "    B = vstack((ones(n),k_vals)).T\n",
    "    logE = log(errors)\n",
    "    \n",
    "    # solve least square problem:\n",
    "    c, residuals, rank, s = lstsq(B,logE,rcond=None)\n",
    "    \n",
    "    logC = c[0]\n",
    "    logrho = c[1]\n",
    "    C = exp(logC)\n",
    "    rho = exp(logrho)\n",
    "    print('Convergence approximately like  E(k) = %.3f * rho**k   with rho = %.8f' % (C,rho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = arange(50, 200, dtype=int)\n",
    "rho_Jacobi = convergence_rate(k_vals, errors_Jacobi[k_vals])\n",
    "\n",
    "# compare with the value expected from (4.16) in the text:\n",
    "rho_theory = 1 - 0.5*pi**2*h**2\n",
    "print('Predicted value of rho = %.8f' % rho_theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = arange(50, 200, dtype=int)\n",
    "rho_GS = convergence_rate(k_vals, errors_GS[k_vals])\n",
    "\n",
    "# compare with the value expected from (4.21) in the text:\n",
    "rho_theory = 1 - pi**2*h**2\n",
    "print('Predicted value of rho = %.8f' % rho_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOR method\n",
    "\n",
    "The successive overrelaxation method computes the G-S update at each point but then moves farther in this direction.  The optimal $\\omega$ for this one-dimensional Poisson problem is given in Section 4.2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_opt = 2 / (1 + sin(pi*h))\n",
    "print('Optimal omega = %.6f' % omega_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the update method for SOR.  You might want to try changing `omega` to different values to see how this affects convergence.  It should converge as long as $0 < \\omega < 2$ but even small changes in $\\omega$ will give much poorer results than the optimal.  (And recall that $\\omega = 1$ corresponds to Gauss-Seidel, so setting $\\omega < 1$ would be *under-relaxed* and converge even slower than G-S.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = omega_opt\n",
    "omega = 1.9\n",
    "\n",
    "def update_u_SOR(U, F, h):\n",
    "    \"\"\"\n",
    "    Input: Current iterate U^[k-1]\n",
    "    Output: Next iterate U^[k]\n",
    "    \"\"\"\n",
    "    m = len(U) - 2\n",
    "    for i in range(1,m+1):\n",
    "        U[i] = 0.5*(U[i-1] + U[i+1] - h**2 * F[i])*omega \\\n",
    "               + (1-omega)*U[i]\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_SOR, figs = solve_bvp(f_fcn, utrue_fcn, m=m, update_u=update_u_SOR, \n",
    "                             maxiter=400, kplot=20)\n",
    "\n",
    "animate_figs(figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogy(errors_Jacobi, 'r', label='Jacobi')\n",
    "semilogy(errors_GS, 'b', label='Gauss-Seidel')\n",
    "semilogy(errors_SOR, 'g', label='SOR')\n",
    "legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = arange(50, 100, dtype=int)\n",
    "rho_SOR = convergence_rate(k_vals, errors_SOR[k_vals])\n",
    "\n",
    "rho_theory = omega - 1  # only correct if omega_opt <= omega < 2\n",
    "print('Predicted value of rho = %.8f' % rho_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a different $f$ function\n",
    "\n",
    "Remember that the iterative method is converging (we hope) to the solution of the linear system of equations, and **not** to the true solution of the differential equation in general.  In the example above we chose a problem for which the truncation error is zero and so the two are ths same, but this is not true in general.\n",
    "\n",
    "Here's a problem for which the numerical approximation to the ODE is not exact, and so if we compute the error as we did above by comparing to the true solution of the ODE it will not go to zero as we take more iterations.  It will stop decreasing once the global error of the finite difference method is greater than the error in our approximation to the solution of the linear system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fcn = lambda x: -12*sin(2*x)\n",
    "utrue_fcn = lambda x: 3*sin(2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_SOR, figs = solve_bvp(f_fcn, utrue_fcn, m=m, update_u=update_u_SOR, \n",
    "                             maxiter=100, kplot=5)\n",
    "\n",
    "animate_figs(figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogy(errors_SOR, 'g', label='SOR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, to get a better approximation to the ODE solution we would of course have to use a finer grid.  Note that for SOR the optimal omega then has to be adjusted as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 39\n",
    "h = 1./(m+1)\n",
    "omega_opt = 2 / (1 + sin(pi*h))\n",
    "print('Optimal omega = %.6f' % omega_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = omega_opt\n",
    "\n",
    "def update_u_SOR(U, F, h):\n",
    "    \"\"\"\n",
    "    Input: Current iterate U^[k-1]\n",
    "    Output: Next iterate U^[k]\n",
    "    \"\"\"\n",
    "    m = len(U) - 2\n",
    "    for i in range(1,m+1):\n",
    "        U[i] = 0.5*(U[i-1] + U[i+1] - h**2 * F[i])*omega \\\n",
    "               + (1-omega)*U[i]\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_SOR, figs = solve_bvp(f_fcn, utrue_fcn, m=39, update_u=update_u_SOR, \n",
    "                             maxiter=200, kplot=10)\n",
    "\n",
    "animate_figs(figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogy(errors_SOR, 'g', label='SOR');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
